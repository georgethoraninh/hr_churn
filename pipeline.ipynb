{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "#plt.style.use('default')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest, VarianceThreshold\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report, recall_score, plot_confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', 250)\n",
    "pd.set_option('display.min_rows', 100)\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_for_preprocessing_ord.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>churn</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>dept</th>\n",
       "      <th>salary</th>\n",
       "      <th>sat_level_cluster</th>\n",
       "      <th>last_eval_cluster</th>\n",
       "      <th>avg_month_hr_cluster</th>\n",
       "      <th>satisfaction_level^1xlast_evaluation^1</th>\n",
       "      <th>satisfaction_level^1xnumber_project^1</th>\n",
       "      <th>satisfaction_level^1xaverage_montly_hours^1</th>\n",
       "      <th>satisfaction_level^1xtime_spend_company^1</th>\n",
       "      <th>last_evaluation^1xnumber_project^1</th>\n",
       "      <th>last_evaluation^1xaverage_montly_hours^1</th>\n",
       "      <th>last_evaluation^1xtime_spend_company^1</th>\n",
       "      <th>number_project^1xaverage_montly_hours^1</th>\n",
       "      <th>number_project^1xtime_spend_company^1</th>\n",
       "      <th>average_montly_hours^1xtime_spend_company^1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.76</td>\n",
       "      <td>59.66</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.06</td>\n",
       "      <td>83.21</td>\n",
       "      <td>1.59</td>\n",
       "      <td>314.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>4.00</td>\n",
       "      <td>209.60</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.30</td>\n",
       "      <td>225.32</td>\n",
       "      <td>5.16</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0968</td>\n",
       "      <td>0.77</td>\n",
       "      <td>29.92</td>\n",
       "      <td>0.44</td>\n",
       "      <td>6.16</td>\n",
       "      <td>239.36</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1904.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>3.60</td>\n",
       "      <td>160.56</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.35</td>\n",
       "      <td>194.01</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.74</td>\n",
       "      <td>58.83</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.04</td>\n",
       "      <td>82.68</td>\n",
       "      <td>1.56</td>\n",
       "      <td>318.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  satisfaction_level  last_evaluation  number_project  \\\n",
       "0   0                0.38             0.53               2   \n",
       "1   1                0.80             0.86               5   \n",
       "2   2                0.11             0.88               7   \n",
       "3   3                0.72             0.87               5   \n",
       "4   4                0.37             0.52               2   \n",
       "\n",
       "   average_montly_hours  time_spend_company  Work_accident  churn  \\\n",
       "0                   157                   3              0      1   \n",
       "1                   262                   6              0      1   \n",
       "2                   272                   4              0      1   \n",
       "3                   223                   5              0      1   \n",
       "4                   159                   3              0      1   \n",
       "\n",
       "   promotion_last_5years   dept  salary  sat_level_cluster  last_eval_cluster  \\\n",
       "0                      0  sales     0.0                1.0                0.0   \n",
       "1                      0  sales     1.0                3.0                3.0   \n",
       "2                      0  sales     1.0                0.0                3.0   \n",
       "3                      0  sales     0.0                2.0                3.0   \n",
       "4                      0  sales     0.0                1.0                0.0   \n",
       "\n",
       "   avg_month_hr_cluster  satisfaction_level^1xlast_evaluation^1  \\\n",
       "0                   0.0                                  0.2014   \n",
       "1                   2.0                                  0.6880   \n",
       "2                   2.0                                  0.0968   \n",
       "3                   1.0                                  0.6264   \n",
       "4                   0.0                                  0.1924   \n",
       "\n",
       "   satisfaction_level^1xnumber_project^1  \\\n",
       "0                                   0.76   \n",
       "1                                   4.00   \n",
       "2                                   0.77   \n",
       "3                                   3.60   \n",
       "4                                   0.74   \n",
       "\n",
       "   satisfaction_level^1xaverage_montly_hours^1  \\\n",
       "0                                        59.66   \n",
       "1                                       209.60   \n",
       "2                                        29.92   \n",
       "3                                       160.56   \n",
       "4                                        58.83   \n",
       "\n",
       "   satisfaction_level^1xtime_spend_company^1  \\\n",
       "0                                       1.14   \n",
       "1                                       4.80   \n",
       "2                                       0.44   \n",
       "3                                       3.60   \n",
       "4                                       1.11   \n",
       "\n",
       "   last_evaluation^1xnumber_project^1  \\\n",
       "0                                1.06   \n",
       "1                                4.30   \n",
       "2                                6.16   \n",
       "3                                4.35   \n",
       "4                                1.04   \n",
       "\n",
       "   last_evaluation^1xaverage_montly_hours^1  \\\n",
       "0                                     83.21   \n",
       "1                                    225.32   \n",
       "2                                    239.36   \n",
       "3                                    194.01   \n",
       "4                                     82.68   \n",
       "\n",
       "   last_evaluation^1xtime_spend_company^1  \\\n",
       "0                                    1.59   \n",
       "1                                    5.16   \n",
       "2                                    3.52   \n",
       "3                                    4.35   \n",
       "4                                    1.56   \n",
       "\n",
       "   number_project^1xaverage_montly_hours^1  \\\n",
       "0                                    314.0   \n",
       "1                                   1310.0   \n",
       "2                                   1904.0   \n",
       "3                                   1115.0   \n",
       "4                                    318.0   \n",
       "\n",
       "   number_project^1xtime_spend_company^1  \\\n",
       "0                                    6.0   \n",
       "1                                   30.0   \n",
       "2                                   28.0   \n",
       "3                                   25.0   \n",
       "4                                    6.0   \n",
       "\n",
       "   average_montly_hours^1xtime_spend_company^1  \n",
       "0                                        471.0  \n",
       "1                                       1572.0  \n",
       "2                                       1088.0  \n",
       "3                                       1115.0  \n",
       "4                                        477.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'satisfaction_level', 'last_evaluation', 'number_project',\n",
       "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'churn',\n",
       "       'promotion_last_5years', 'dept', 'salary', 'sat_level_cluster',\n",
       "       'last_eval_cluster', 'avg_month_hr_cluster',\n",
       "       'satisfaction_level^1xlast_evaluation^1',\n",
       "       'satisfaction_level^1xnumber_project^1',\n",
       "       'satisfaction_level^1xaverage_montly_hours^1',\n",
       "       'satisfaction_level^1xtime_spend_company^1',\n",
       "       'last_evaluation^1xnumber_project^1',\n",
       "       'last_evaluation^1xaverage_montly_hours^1',\n",
       "       'last_evaluation^1xtime_spend_company^1',\n",
       "       'number_project^1xaverage_montly_hours^1',\n",
       "       'number_project^1xtime_spend_company^1',\n",
       "       'average_montly_hours^1xtime_spend_company^1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/7-data-types-a-better-way-to-think-about-data-types-for-machine-learning-939fae99a689\n",
    "\n",
    "#For personal purposes\n",
    "\n",
    "useless_var = ['id']\n",
    "\n",
    "nominal_var = ['dept']\n",
    "\n",
    "ordinal_var = ['salary',\n",
    "               'sat_level_cluster',\n",
    "               'last_eval_cluster',\n",
    "               'avg_month_hr_cluster']\n",
    "\n",
    "binary_var = ['Work_accident',\n",
    "              'promotion_last_5years']\n",
    "\n",
    "target_var = ['churn']\n",
    "\n",
    "count_var = ['number_project',\n",
    "             'time_spend_company']\n",
    "\n",
    "interval_var = ['satisfaction_level',\n",
    "                'last_evaluation',\n",
    "                'average_montly_hours',\n",
    "                'satisfaction_level^1xlast_evaluation^1',\n",
    "                'satisfaction_level^1xnumber_project^1',\n",
    "                'satisfaction_level^1xaverage_montly_hours^1',\n",
    "                'satisfaction_level^1xtime_spend_company^1',\n",
    "                'last_evaluation^1xnumber_project^1',\n",
    "                'last_evaluation^1xaverage_montly_hours^1',\n",
    "                'last_evaluation^1xtime_spend_company^1',\n",
    "                'number_project^1xaverage_montly_hours^1',\n",
    "                'number_project^1xtime_spend_company^1',\n",
    "                'average_montly_hours^1xtime_spend_company^1']\n",
    "\n",
    "num_var = count_var + interval_var\n",
    "cat_var = nominal_var + ordinal_var + binary_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[\n",
    "    \n",
    "    ('features', FeatureUnion([\n",
    "        \n",
    "        ('numerical_features', ColumnTransformer([\n",
    "            ('numerical', Pipeline(steps=[\n",
    "                ('min_max_scale', MinMaxScaler()),\n",
    "                ('num_selection', VarianceThreshold(threshold = 0.05))]),\n",
    "            ['number_project', 'time_spend_company', 'satisfaction_level', 'last_evaluation',                                  'average_montly_hours', 'satisfaction_level^1xlast_evaluation^1',                                                  'satisfaction_level^1xnumber_project^1', 'satisfaction_level^1xaverage_montly_hours^1',                            'satisfaction_level^1xtime_spend_company^1', 'last_evaluation^1xnumber_project^1',                                'last_evaluation^1xaverage_montly_hours^1', 'last_evaluation^1xtime_spend_company^1',                              'number_project^1xaverage_montly_hours^1', 'number_project^1xtime_spend_company^1',                                'average_montly_hours^1xtime_spend_company^1'])\n",
    "        ])),\n",
    "        \n",
    "        ('categorical_features', ColumnTransformer([\n",
    "            ('categorical', Pipeline(steps=[\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "                (\"cat_selection\", SelectKBest(score_func = chi2, k = 10))]),\n",
    "            ['dept'])\n",
    "        ],\n",
    "        remainder = 'passthrough'))\n",
    "    ])),\n",
    "    \n",
    "    #('classifiers', xgb.XGBClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helen Tip:\n",
    "pipeline.named_steps['feature_selection_thing'] will show you your select K best thing.\n",
    "\n",
    "Add onto that as if it's the feature selector to find what you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      3428\n",
      "           1       0.97      0.91      0.94      1072\n",
      "\n",
      "    accuracy                           0.97      4500\n",
      "   macro avg       0.97      0.95      0.96      4500\n",
      "weighted avg       0.97      0.97      0.97      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_pipeline.fit(X_train, y_train)\n",
    "print(classification_report(y_test,model_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': range(60, 220, 40),\n",
    "    'learning_rate': [0.1, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter learning_rate for estimator Pipeline(memory=None,\n         steps=[('min_max_scale',\n                 MinMaxScaler(copy=True, feature_range=(0, 1))),\n                ('num_selection', VarianceThreshold(threshold=0.05)),\n                ('xg_boost',\n                 XGBClassifier(base_score=0.5, booster='gbtree',\n                               colsample_bylevel=1, colsample_bynode=1,\n                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n                               max_delta_step=0, max_depth=3,\n                               min_child_weight=1, missing=None,\n                               n_estimators=100, n_jobs=1, nthread=None,\n                               objective='binary:logistic', random_state=42,\n                               reg_alpha=0, reg_lambda=1, scale_pos_weight=3.2,\n                               seed=None, silent=None, subsample=1,\n                               verbosity=0))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-bc00057b30d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcv_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcv_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfinal_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfinal_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xg_boost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_set_params\u001b[0;34m(self, attr, **params)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# 3. Step parameters and other initialisation arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/unit_5/unit5_env/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    234\u001b[0m                                  \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                                  \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                                  (key, self))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter learning_rate for estimator Pipeline(memory=None,\n         steps=[('min_max_scale',\n                 MinMaxScaler(copy=True, feature_range=(0, 1))),\n                ('num_selection', VarianceThreshold(threshold=0.05)),\n                ('xg_boost',\n                 XGBClassifier(base_score=0.5, booster='gbtree',\n                               colsample_bylevel=1, colsample_bynode=1,\n                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n                               max_delta_step=0, max_depth=3,\n                               min_child_weight=1, missing=None,\n                               n_estimators=100, n_jobs=1, nthread=None,\n                               objective='binary:logistic', random_state=42,\n                               reg_alpha=0, reg_lambda=1, scale_pos_weight=3.2,\n                               seed=None, silent=None, subsample=1,\n                               verbosity=0))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    ('min_max_scale', MinMaxScaler()),\n",
    "    ('num_selection', VarianceThreshold(threshold = 0.05)),\n",
    "    ('xg_boost', xgb.XGBClassifier(scale_pos_weight=3.2,verbosity=0,random_state=42))\n",
    "    ])\n",
    "\n",
    "\n",
    "cv_pipeline = GridSearchCV(estimator=num_pipeline, param_grid=parameters)\n",
    "cv_pipeline.fit(X=X_train, y=y_train)\n",
    "final_pipeline = cv_pipeline.best_estimator_\n",
    "final_classifier = final_pipeline.named_steps['xg_boost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_score', 'booster', 'colsample_bylevel', 'colsample_bynode', 'colsample_bytree', 'gamma', 'learning_rate', 'max_delta_step', 'max_depth', 'min_child_weight', 'missing', 'n_estimators', 'n_jobs', 'nthread', 'objective', 'random_state', 'reg_alpha', 'reg_lambda', 'scale_pos_weight', 'seed', 'silent', 'subsample', 'verbosity'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    (\"cat_selection\", SelectKBest(score_func = chi2, k = 10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers = [\n",
    "                                                ('numerical', num_pipeline, num_var),\n",
    "                                                ('categorical', cat_pipeline, nominal_var)\n",
    "                                                ],\n",
    "                                 remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables\n",
    "X = df[num_var + cat_var]\n",
    "y = df[target_var]\n",
    "\n",
    "# Then split the remaining into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_transformer = OneHotEncoder([['sales', 'accounting', 'hr', 'technical', 'support', 'management',\n",
    "       'IT', 'product_mng', 'marketing', 'RandD'],\n",
    "                                    ['low', 'medium', 'high'],\n",
    "                                    ['unsatisfied', 'very satisfied', 'very unsatisfied', 'satisfied'],\n",
    "                                    ['low performance', 'high performance', 'very high performance',\n",
    "       'average performance'],\n",
    "                                    ['low', 'high', 'medium']\n",
    "                                    ])\n",
    "\n",
    "num_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers = [\n",
    "                                                ('scaler', num_transformer, num_var),\n",
    "                                                ('onehot', nominal_transformer, ordinal_var)\n",
    "                                                ],\n",
    "                                remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables\n",
    "X = df[num_var + cat_var]\n",
    "y = df[target_var]\n",
    "\n",
    "# Then split the remaining into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10499, 22)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the preprocessor\n",
    "X_train_transformed = preprocessor.fit_transform(X_train,y_train)\n",
    "\n",
    "# Transform the input validation and test sets\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['numerical', 'categorical', 'remainder'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why we name the transformers\n",
    "preprocessor.named_transformers_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all the transformed X data back into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numerical': Pipeline(memory=None,\n",
       "          steps=[('min_max_scale',\n",
       "                  MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                 ('num_selection', VarianceThreshold(threshold=0.05))],\n",
       "          verbose=False),\n",
       " 'categorical': Pipeline(memory=None,\n",
       "          steps=[('onehot',\n",
       "                  OneHotEncoder(categories='auto', drop=None,\n",
       "                                dtype=<class 'numpy.float64'>,\n",
       "                                handle_unknown='ignore', sparse=True)),\n",
       "                 ('cat_selection',\n",
       "                  SelectKBest(k=10,\n",
       "                              score_func=<function chi2 at 0x7f885115f378>))],\n",
       "          verbose=False),\n",
       " 'remainder': 'passthrough'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method OneHotEncoder.get_feature_names of OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='ignore', sparse=True)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grab the column names for all of our nominal variables\n",
    "preprocessor.named_transformers_.categorical.named_steps.onehot.get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method OneHotEncoder.get_feature_names of OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='ignore', sparse=True)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-61ea18803614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_var\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_columns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbinary_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "transformed_columns = num_var + list(one_hot_columns) + binary_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10499, 41)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape # Great! They match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform_df = pd.DataFrame(X_train_transformed, columns = transformed_columns)\n",
    "X_test_transform_df = pd.DataFrame(X_test_transformed, columns = transformed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=42, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95      8000\n",
      "           1       0.79      0.92      0.85      2499\n",
      "\n",
      "    accuracy                           0.92     10499\n",
      "   macro avg       0.88      0.92      0.90     10499\n",
      "weighted avg       0.93      0.92      0.93     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      3428\n",
      "           1       0.77      0.90      0.83      1072\n",
      "\n",
      "    accuracy                           0.91      4500\n",
      "   macro avg       0.87      0.91      0.89      4500\n",
      "weighted avg       0.92      0.91      0.91      4500\n",
      "\n",
      "*****DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=42, splitter='best')*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8000\n",
      "           1       1.00      1.00      1.00      2499\n",
      "\n",
      "    accuracy                           1.00     10499\n",
      "   macro avg       1.00      1.00      1.00     10499\n",
      "weighted avg       1.00      1.00      1.00     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      3428\n",
      "           1       0.94      0.95      0.95      1072\n",
      "\n",
      "    accuracy                           0.97      4500\n",
      "   macro avg       0.96      0.97      0.97      4500\n",
      "weighted avg       0.97      0.97      0.97      4500\n",
      "\n",
      "*****DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=42, splitter='best')*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8000\n",
      "           1       1.00      1.00      1.00      2499\n",
      "\n",
      "    accuracy                           1.00     10499\n",
      "   macro avg       1.00      1.00      1.00     10499\n",
      "weighted avg       1.00      1.00      1.00     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      3428\n",
      "           1       0.94      0.96      0.95      1072\n",
      "\n",
      "    accuracy                           0.98      4500\n",
      "   macro avg       0.96      0.97      0.97      4500\n",
      "weighted avg       0.98      0.98      0.98      4500\n",
      "\n",
      "*****RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8000\n",
      "           1       1.00      1.00      1.00      2499\n",
      "\n",
      "    accuracy                           1.00     10499\n",
      "   macro avg       1.00      1.00      1.00     10499\n",
      "weighted avg       1.00      1.00      1.00     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3428\n",
      "           1       0.99      0.95      0.97      1072\n",
      "\n",
      "    accuracy                           0.99      4500\n",
      "   macro avg       0.99      0.97      0.98      4500\n",
      "weighted avg       0.99      0.99      0.99      4500\n",
      "\n",
      "*****AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      8000\n",
      "           1       0.95      0.94      0.94      2499\n",
      "\n",
      "    accuracy                           0.97     10499\n",
      "   macro avg       0.96      0.96      0.96     10499\n",
      "weighted avg       0.97      0.97      0.97     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      3428\n",
      "           1       0.93      0.93      0.93      1072\n",
      "\n",
      "    accuracy                           0.97      4500\n",
      "   macro avg       0.95      0.95      0.95      4500\n",
      "weighted avg       0.97      0.97      0.97      4500\n",
      "\n",
      "*****GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      8000\n",
      "           1       0.99      0.94      0.96      2499\n",
      "\n",
      "    accuracy                           0.98     10499\n",
      "   macro avg       0.98      0.97      0.97     10499\n",
      "weighted avg       0.98      0.98      0.98     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      3428\n",
      "           1       0.97      0.92      0.94      1072\n",
      "\n",
      "    accuracy                           0.97      4500\n",
      "   macro avg       0.97      0.95      0.96      4500\n",
      "weighted avg       0.97      0.97      0.97      4500\n",
      "\n",
      "*****XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=3.2, seed=None,\n",
      "              silent=None, subsample=1, verbosity=0)*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      8000\n",
      "           1       0.95      0.94      0.95      2499\n",
      "\n",
      "    accuracy                           0.97     10499\n",
      "   macro avg       0.96      0.96      0.96     10499\n",
      "weighted avg       0.97      0.97      0.97     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      3428\n",
      "           1       0.94      0.92      0.93      1072\n",
      "\n",
      "    accuracy                           0.97      4500\n",
      "   macro avg       0.96      0.95      0.95      4500\n",
      "weighted avg       0.97      0.97      0.97      4500\n",
      "\n",
      "*****<catboost.core.CatBoostClassifier object at 0x7fe3c09a9b70>*****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8000\n",
      "           1       0.99      1.00      0.99      2499\n",
      "\n",
      "    accuracy                           1.00     10499\n",
      "   macro avg       1.00      1.00      1.00     10499\n",
      "weighted avg       1.00      1.00      1.00     10499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3428\n",
      "           1       0.98      0.96      0.97      1072\n",
      "\n",
      "    accuracy                           0.99      4500\n",
      "   macro avg       0.98      0.98      0.98      4500\n",
      "weighted avg       0.99      0.99      0.99      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    LogisticRegression(penalty = 'l1', solver='saga', class_weight='balanced',random_state=42, C=1),\n",
    "    #SVC(kernel=\"rbf\", C=0.025, random_state=42, probability=True, class_weight='balanced'),\n",
    "    DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    #VotingClassifier(random_state=42),\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    xgb.XGBClassifier(scale_pos_weight=3.2,verbosity=0,random_state=42),\n",
    "    CatBoostClassifier(class_weights=[1,3.2],verbose=False,random_seed=42)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('classifier', classifier)])\n",
    "    pipe.fit(X_train_transform_df, y_train)\n",
    "    print(f'*****{classifier}*****')\n",
    "    print(classification_report(y_train,pipe.predict(X_train_transform_df)))\n",
    "    print(classification_report(y_test,pipe.predict(X_test_transform_df)))\n",
    "    #print(\"model recall score: %.3f\" % pipe.score(X_test_transform_df, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we want to minimize the number of FN (Employees who are incorrectly predicted to not churn; they were actually supposed to churn), so we want a better recall score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
